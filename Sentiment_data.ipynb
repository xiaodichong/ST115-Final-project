{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "103f445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw # importing praw to access reddit api\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk # importing nltk to acc\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0b6be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('keys.json') as f:\n",
    "    keys = json.load(f)\n",
    "\n",
    "app_id = keys['reddit']['app_id']\n",
    "app_secret = keys['reddit']['app_secret']\n",
    "username = keys['reddit']['username']\n",
    "password = keys['reddit']['password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96bc4380",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id = app_id,\n",
    "                    client_secret = app_secret,\n",
    "                    user_agent = f'lse/0.0.1 by {username}',\n",
    "                    username = username,\n",
    "                    password = password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44a0117c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "specific_model = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88684de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a95e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    compound_scores = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentiment = analyzer.polarity_scores(sentence)\n",
    "        compound_scores.append(sentiment['compound'])\n",
    "\n",
    "    if compound_scores:\n",
    "        avg_compound_score = np.mean(compound_scores)\n",
    "    else:\n",
    "        avg_compound_score = 0\n",
    "\n",
    "    return avg_compound_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31c8bbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "import emoji\n",
    "\n",
    "def remove_emojis(text):\n",
    "    return emoji.get_emoji_regexp().sub(u'', text)\n",
    "\n",
    "subreddit = reddit.subreddit(\"6thForm\")\n",
    "\n",
    "post_text_lst= []\n",
    "comment_text_lst = []\n",
    "\n",
    "for index, submission in enumerate(subreddit.search(\"LSE+Lse+lse\", limit=1000,sort=\"relevance\")):\n",
    "# for index, submission in enumerate(subreddit.search(\"UCL+ucl+Ucl\", limit=1000,sort=\"relevance\")):\n",
    "# for index, submission in enumerate(subreddit.search(\"Imperial+IMPERIAL+imperial+icl\", limit=10,sort=\"relevance\")):\n",
    "# for index, submission in enumerate(subreddit.search(\"Kings+KINGS+KCL+Kcl+kcl\", limit=1000,sort=\"relevance\")):\n",
    "# for index, submission in enumerate(subreddit.search(\"Cambridge+Oxford+Oxbridge\", limit=1000,sort=\"relevance\")):\n",
    "    if submission.link_flair_text is not None:\n",
    "        post_flair = remove_emojis(submission.link_flair_text)\n",
    "    else:\n",
    "        post_flair = None\n",
    "    \n",
    "    post_text_lst.append((index, remove_emojis(submission.selftext),post_flair, \"post_text\", datetime.utcfromtimestamp(submission.created_utc)))\n",
    "    if submission.num_comments > 0:\n",
    "        comments = submission.comments.replace_more(limit=0)            \n",
    "        for comments in submission.comments.list():\n",
    "            if comments.author != \"AutoModerator\":\n",
    "                comment_text_lst.append((index, remove_emojis(comments.body), post_flair, \"comment_text\", datetime.utcfromtimestamp(comments.created_utc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eb4a42f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_index</th>\n",
       "      <th>text</th>\n",
       "      <th>flair</th>\n",
       "      <th>post_type</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Its coming up to deadline and has anyone else ...</td>\n",
       "      <td>UNI / UCAS</td>\n",
       "      <td>post_text</td>\n",
       "      <td>2024-04-12 11:31:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I’ve heard that many people are dying to get a...</td>\n",
       "      <td>UNI / UCAS</td>\n",
       "      <td>post_text</td>\n",
       "      <td>2022-12-14 07:39:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I know there isn't a conclusive answer anybody...</td>\n",
       "      <td>UNI / UCAS</td>\n",
       "      <td>post_text</td>\n",
       "      <td>2024-04-11 13:56:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A* A* A A(epq) predictions, 31 LNAT\\ngot rejec...</td>\n",
       "      <td>UNI / UCAS</td>\n",
       "      <td>post_text</td>\n",
       "      <td>2024-04-26 19:40:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>LSE pure econ :((((((((\\n\\nPlease give me tips...</td>\n",
       "      <td>DISCUSSION</td>\n",
       "      <td>post_text</td>\n",
       "      <td>2024-04-25 11:04:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5398</th>\n",
       "      <td>246</td>\n",
       "      <td>Lol, look up what a normal distribution looks ...</td>\n",
       "      <td>UNI / UCAS</td>\n",
       "      <td>comment_text</td>\n",
       "      <td>2023-12-09 18:19:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5399</th>\n",
       "      <td>246</td>\n",
       "      <td>Mhm cos thats why they dont know how averages ...</td>\n",
       "      <td>UNI / UCAS</td>\n",
       "      <td>comment_text</td>\n",
       "      <td>2023-12-08 18:55:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5400</th>\n",
       "      <td>246</td>\n",
       "      <td>Yea and the normal distribution in this case h...</td>\n",
       "      <td>UNI / UCAS</td>\n",
       "      <td>comment_text</td>\n",
       "      <td>2023-12-09 18:43:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5401</th>\n",
       "      <td>246</td>\n",
       "      <td>how do you know that, you realise also that di...</td>\n",
       "      <td>UNI / UCAS</td>\n",
       "      <td>comment_text</td>\n",
       "      <td>2023-12-09 18:52:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5402</th>\n",
       "      <td>246</td>\n",
       "      <td>Because each score from 6.2-7 only has like 3%...</td>\n",
       "      <td>UNI / UCAS</td>\n",
       "      <td>comment_text</td>\n",
       "      <td>2023-12-09 18:53:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5403 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      post_index                                               text  \\\n",
       "0              0  Its coming up to deadline and has anyone else ...   \n",
       "1              1  I’ve heard that many people are dying to get a...   \n",
       "2              2  I know there isn't a conclusive answer anybody...   \n",
       "3              3  A* A* A A(epq) predictions, 31 LNAT\\ngot rejec...   \n",
       "4              4  LSE pure econ :((((((((\\n\\nPlease give me tips...   \n",
       "...          ...                                                ...   \n",
       "5398         246  Lol, look up what a normal distribution looks ...   \n",
       "5399         246  Mhm cos thats why they dont know how averages ...   \n",
       "5400         246  Yea and the normal distribution in this case h...   \n",
       "5401         246  how do you know that, you realise also that di...   \n",
       "5402         246  Because each score from 6.2-7 only has like 3%...   \n",
       "\n",
       "            flair     post_type                date  \n",
       "0      UNI / UCAS     post_text 2024-04-12 11:31:40  \n",
       "1      UNI / UCAS     post_text 2022-12-14 07:39:30  \n",
       "2      UNI / UCAS     post_text 2024-04-11 13:56:05  \n",
       "3      UNI / UCAS     post_text 2024-04-26 19:40:48  \n",
       "4      DISCUSSION     post_text 2024-04-25 11:04:22  \n",
       "...           ...           ...                 ...  \n",
       "5398   UNI / UCAS  comment_text 2023-12-09 18:19:11  \n",
       "5399   UNI / UCAS  comment_text 2023-12-08 18:55:39  \n",
       "5400   UNI / UCAS  comment_text 2023-12-09 18:43:20  \n",
       "5401   UNI / UCAS  comment_text 2023-12-09 18:52:55  \n",
       "5402   UNI / UCAS  comment_text 2023-12-09 18:53:46  \n",
       "\n",
       "[5403 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(post_text_lst + comment_text_lst, columns=['post_index', 'text', 'flair', 'post_type', \"date\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8140d91",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_index</th>\n",
       "      <th>text</th>\n",
       "      <th>flair</th>\n",
       "      <th>post_type</th>\n",
       "      <th>date</th>\n",
       "      <th>Word_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Its coming up to deadline and has anyone else ...</td>\n",
       "      <td>UNI / UCAS</td>\n",
       "      <td>post_text</td>\n",
       "      <td>2024-04-12 11:31:40</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm wondering if I should send them an email a...</td>\n",
       "      <td>UNI / UCAS</td>\n",
       "      <td>post_text</td>\n",
       "      <td>2024-04-12 11:31:40</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I’ve heard that many people are dying to get a...</td>\n",
       "      <td>UNI / UCAS</td>\n",
       "      <td>post_text</td>\n",
       "      <td>2022-12-14 07:39:30</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I know there isn't a conclusive answer anybody...</td>\n",
       "      <td>UNI / UCAS</td>\n",
       "      <td>post_text</td>\n",
       "      <td>2024-04-11 13:56:05</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I applied in October for econ and have been ac...</td>\n",
       "      <td>UNI / UCAS</td>\n",
       "      <td>post_text</td>\n",
       "      <td>2024-04-11 13:56:05</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5400</th>\n",
       "      <td>246</td>\n",
       "      <td>Yea and the normal distribution in this case h...</td>\n",
       "      <td>UNI / UCAS</td>\n",
       "      <td>comment_text</td>\n",
       "      <td>2023-12-09 18:43:20</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5401</th>\n",
       "      <td>246</td>\n",
       "      <td>how do you know that, you realise also that di...</td>\n",
       "      <td>UNI / UCAS</td>\n",
       "      <td>comment_text</td>\n",
       "      <td>2023-12-09 18:52:55</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5401</th>\n",
       "      <td>246</td>\n",
       "      <td>That could just be rng remember its à multiple...</td>\n",
       "      <td>UNI / UCAS</td>\n",
       "      <td>comment_text</td>\n",
       "      <td>2023-12-09 18:52:55</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5401</th>\n",
       "      <td>246</td>\n",
       "      <td>Further looking at the tmua distribution it’s ...</td>\n",
       "      <td>UNI / UCAS</td>\n",
       "      <td>comment_text</td>\n",
       "      <td>2023-12-09 18:52:55</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5402</th>\n",
       "      <td>246</td>\n",
       "      <td>Because each score from 6.2-7 only has like 3%...</td>\n",
       "      <td>UNI / UCAS</td>\n",
       "      <td>comment_text</td>\n",
       "      <td>2023-12-09 18:53:46</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4278 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      post_index                                               text  \\\n",
       "0              0  Its coming up to deadline and has anyone else ...   \n",
       "0              0  I'm wondering if I should send them an email a...   \n",
       "1              1  I’ve heard that many people are dying to get a...   \n",
       "2              2  I know there isn't a conclusive answer anybody...   \n",
       "2              2  I applied in October for econ and have been ac...   \n",
       "...          ...                                                ...   \n",
       "5400         246  Yea and the normal distribution in this case h...   \n",
       "5401         246  how do you know that, you realise also that di...   \n",
       "5401         246  That could just be rng remember its à multiple...   \n",
       "5401         246  Further looking at the tmua distribution it’s ...   \n",
       "5402         246  Because each score from 6.2-7 only has like 3%...   \n",
       "\n",
       "            flair     post_type                date  Word_Count  \n",
       "0      UNI / UCAS     post_text 2024-04-12 11:31:40          53  \n",
       "0      UNI / UCAS     post_text 2024-04-12 11:31:40          26  \n",
       "1      UNI / UCAS     post_text 2022-12-14 07:39:30          27  \n",
       "2      UNI / UCAS     post_text 2024-04-11 13:56:05          28  \n",
       "2      UNI / UCAS     post_text 2024-04-11 13:56:05          40  \n",
       "...           ...           ...                 ...         ...  \n",
       "5400   UNI / UCAS  comment_text 2023-12-09 18:43:20          18  \n",
       "5401   UNI / UCAS  comment_text 2023-12-09 18:52:55          20  \n",
       "5401   UNI / UCAS  comment_text 2023-12-09 18:52:55          11  \n",
       "5401   UNI / UCAS  comment_text 2023-12-09 18:52:55          17  \n",
       "5402   UNI / UCAS  comment_text 2023-12-09 18:53:46          19  \n",
       "\n",
       "[4278 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df[df['text']!=\"\"].copy()\n",
    "df2['text'] = df2['text'].str.split(r'\\.\\s+|\\n+')\n",
    "df_exploded = df2.explode('text')\n",
    "df_exploded = df_exploded[df_exploded['text']!=\"\"].copy()\n",
    "df_exploded['Word_Count'] = df_exploded['text'].apply(lambda x: len(str(x).split()))\n",
    "df_exploded = df_exploded[df_exploded['Word_Count'] >= 10]\n",
    "df_exploded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0900ea79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_exploded['sentiment'] = df_exploded['text'].apply(lambda x: specific_model(x, truncation=True, padding=True, max_length=128)[0])\n",
    "df_exploded['vader_compound_score'] = df_exploded['text'].apply(lambda x: analyze_sentiment(x))\n",
    "df_exploded.to_csv('data/lse_sentiment_data.csv', index=False)\n",
    "# df_exploded.to_csv('data/ucl_sentiment_data.csv', index=False)\n",
    "# df_exploded.to_csv('data/imperial_sentiment_data.csv', index=False)\n",
    "# df_exploded.to_csv('data/kings_sentiment_data.csv', index=False)\n",
    "# df_exploded.to_csv('data/oxbridge_sentiment_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
